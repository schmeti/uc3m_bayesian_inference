---
title: "BreastCancer_CaseStudy"
output: pdf_document
date: "2025-02-27"
---

```{r warning=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

This case study is based on the Breast Cancer Wisconsin (Diagnostic) Data Set (https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data). The data set contains 569 observations and 32 variables. The data set is available at the UCI Machine Learning Repository. The data set contains mean (and at times min and max) values of the patient for the following numeric (continious) variables:

a) radius (mean of distances from center to points on the perimeter)
b) texture (standard deviation of gray-scale values)
c) perimeter
d) area
e) smoothness (local variation in radius lengths)
f) compactness (perimeter^2 / area - 1.0)
g) concavity (severity of concave portions of the contour)
h) concave points (number of concave portions of the contour)
i) symmetry
j) fractal dimension ("coastline approximation" - 1)

The data set also contains the following Binary variables:

2) Diagnosis (M = malignant, B = benign)

Where Malignant (M) means the tumor is cancerous, while Benign (B): means that the tumor is non-cancerous.


## Read Data
```{r}
data <- read.csv("data.csv", header = TRUE, sep = ",")
data <- dplyr::select(data, -X)
names(data) <- gsub("\\.", "_", names(data))
data$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
data <- data %>%  select(-id)

```

# ETL - Variable selection

## Relation with response var

```{r, fig.align='left'}
numeric_vars <- data %>%select_if(is.numeric) %>% colnames()

plots <- lapply(numeric_vars, function(var) {
  ggplot(data, aes(x = factor(diagnosis), y = .data[[var]], fill = diagnosis)) + 
    geom_boxplot() +  
    labs(x = "Diagnosis", y = var) +
    theme_minimal()
})

# Print all plots
# install.packages("gridExtra")
library(gridExtra)

# Display plots in batches of 6 (2 rows Ã— 3 columns)
num_plots <- length(plots)
batch_size <- 6

for(i in seq(1, num_plots, batch_size)) {
  end_idx <- min(i + batch_size - 1, num_plots)
  batch_plots <- plots[i:end_idx]
  grid.arrange(grobs = batch_plots, ncol = 3)
}
```

```{r}
library(dplyr)
library(tidyr)

summary_stat <- data %>% 
  group_by(factor(diagnosis)) %>% 
  summarise(across(all_of(numeric_vars), 
                  list(
                    mean = ~mean(.x, na.rm = TRUE),
                    sd = ~sd(.x, na.rm = TRUE),
                    median = ~median(.x, na.rm = TRUE),
                    min = ~min(.x, na.rm = TRUE),
                    max = ~max(.x, na.rm = TRUE)
                  )
            ))


# For easier viewing, you can pivot longer
summary_long <- summary_stat %>%
  pivot_longer(cols = -`factor(diagnosis)`, 
               names_to = c("variable", "stat"), 
               names_pattern = "(.*)_(.*)")

summary_long
```

## Correlation

```{r}
# Check correlation between numeric variables
library(corrplot)
cor_matrix <- cor(data[, numeric_vars])
corrplot(cor_matrix, method = "circle")

# Or find highly correlated variables
high_cor <- findCorrelation(cor_matrix, cutoff = 0.8)
problematic_vars <- numeric_vars[high_cor]
print(problematic_vars)
```

# Variable Selection
<!-- SILVANA -->

## Frequentist Approach

Check VIF and remove variables with extremly high values

```{r}
library(car)
predictors <- setdiff(names(data), c("diagnosis"))
formula_str <- paste("diagnosis ~", paste(predictors, collapse = " + "))
formula <- as.formula(formula_str)

l_reg = lm(formula, data)
vif_values <- vif(l_reg)

vif_df <- data.frame(
  Variable = names(vif_values),
  VIF = vif_values
)
vif_df <- vif_df %>% arrange(desc(VIF))
print(vif_df)

vars_to_exclude <- c(head(vif_df,15)$Variable)
# setdiff(problematic_vars , vars_to_exclude)
# setdiff(vars_to_exclude, problematic_vars)
```

Check correlations after excluding x VIF, variables to pay attention if something does not work.

```{r}
# Check correlation between numeric variables
library(corrplot)
cor_matrix_f <- cor(data[, setdiff(numeric_vars, vars_to_exclude)])
corrplot(cor_matrix, method = "circle")

# Or find highly correlated variables
high_cor_f <- findCorrelation(cor_matrix_f, cutoff = 0.8)
problematic_vars_f <- setdiff(numeric_vars, vars_to_exclude)[high_cor_f]
print(problematic_vars_f)

selected_freq <- setdiff(numeric_vars, vars_to_exclude)
```

## Bayesian Approach

```{r}
library(BAS)

# Fit a Bayesian logistic regression with variable selection
model_bas <- bas.glm(diagnosis ~ ., 
                     data = data,
                     family = binomial(),
                     method = "MCMC", # or "BAS" for deterministic sampling
                     MCMC.iterations = 10000,
                     modelprior = uniform()) # Prior over model space

# Summary of results
summary(model_bas)

# Posterior inclusion probabilities
pip <- model_bas$probne0
variable_names <- names(pip)
pip_df <- data.frame(Variable = numeric_vars, 
                     InclusionProb = pip)
pip_df <- pip_df[order(pip_df$InclusionProb, decreasing = TRUE),]
print(pip_df)

selected_bayes <- c( "perimeter_mean", "concave_points_mean", "compactness_mean",
                     "concavity_mean", "area_se", "smoothness_se", "concave_points_se",
                     "fractal_dimension_se","radius_worst", "texture_worst",
                     "fractal_dimension_worst")
```

# Logistic Models

## Freq var selection

```{r warning=FALSE}
formula_str <- paste("diagnosis ~", paste(selected_freq, collapse = " + "))
formula <- as.formula(formula_str)

freq_model1<-lm(formula, data = data)
beta.start1 <- coef(freq_model1)

out = MCMClogit(formula, data, burnin=1000, mcmc=21000, beta.start = beta.start1)
summary(out)

acf(out[,1])
acf(out[,2])

# Correct autocorrelation

out = MCMClogit(formula, data, burnin=5000, mcmc=30000, thin = 30,
                beta.start = beta.start1)
summary(out)
acf(out[,1])
acf(out[,2])

```

```{r, fig.width=8, fig.height=6, fig.align='left'}
plot(out)
```
## Bayes var selection

```{r}
formula_str_b <- paste("diagnosis ~", paste(selected_bayes, collapse = " + "))
formula_b <- as.formula(formula_str_b)

# starting point
freq_model<-lm(formula_b, data = data)
beta.start <- coef(freq_model)

out_b = MCMClogit(formula_b, data, burnin=1000, mcmc=21000)
summary(out_b)
acf(out_b[,1])
acf(out_b[,2])

out_b = MCMClogit(formula_b, data, burnin=5000, mcmc=50000, 
                  beta.start = beta.start, thin = 50, tune=0.5)

summary(out_b)
acf(out_b[,1])
acf(out_b[,2])
```


```{r, fig.width=8, fig.height=6, fig.align='left'}
plot(out_b)
```

```{r}
#install.packages("MCMCpack")  # If not installed
library(MCMCpack)

# Run Bayesian logistic regression with Spike-and-Slab priors (to variable selection)
set.seed(123)
model <- MCMClogit(formula, data = data, burnin = 5000, mcmc = 20000
                   , marginal.likelihood = "Laplace") # , thin = 10, b0 = 0, B0 = 0.1

summary(model)

apply(model, 2, function(x) mean(x != 0))  # Approximate inclusion probability
```



# Simon

```{r}
# Create a correlation matrix between diagnosis and numeric variables
cor_diagnosis <- sapply(data[, numeric_vars], function(x) cor(as.numeric(data$diagnosis), x))
high_corr_vars <- names(cor_diagnosis)[abs(cor_diagnosis) > 0.7]

model_formula <- as.formula(paste("diagnosis ~", paste(high_corr_vars, collapse = " + ")))
model_formula
```


```{r,warning=FALSE}
# Fit the frequentist logistic regression model
library(MASS)
logit_frequentist <- glm(model_formula, data = data, family = binomial)
step_model <- stepAIC(logit_frequentist, direction = "both")

summary(step_model)
```






```{r}

# Now use Frequentist model formular and parameters starting values in MCMClogit

step_AIC_formula = formula(step_model)
beta.start <- coef(step_model)

logit_bayesian <- MCMClogit(diagnosis ~ concave_points_mean + radius_worst + perimeter_worst, 
                        burnin = 1000, mcmc = 10000, 
                        data = data,lag=20 )#, beta.start = beta.start)

acf(logit_bayesian)
summary(logit_bayesian)

```

```{r}
# DIC Code


model = logit_bayesian
X <- model.matrix(~ concave_points_mean+radius_worst+perimeter_worst, data = data) # model matrix

dev = 0
for (i in 1:nrow(model)) {
  params <- model[i,]
  p = inv.logit(X %*% params)
  p[data$target == 0] = 1-p[data$target == 0]
  dev = dev - 2 * sum(log(p))  # Negative log-likelihood
}
D_bar = dev / nrow(model)

# D_theta: Deviance at the posterior mean (using the average parameter values)
posterior_means <- colMeans(model)
#X <- model.matrix(~ predictors, data = data)
linear_predictor <- X %*% posterior_means
p_post <- inv.logit(linear_predictor)
p_post[data$target == 0] = 1-p_post[data$target == 0]

D_theta = -2 * sum(log(p_post)) # Deviance at the posterior mean

# p_D: Posterior deviance penalty
p_D = D_bar - D_theta

# DIC
DIC = D_bar + p_D

print(list(DIC=DIC, D_bar=D_bar, p_D=p_D))
```




```{r}
# DIC Function
DIC = function(model, predictors, data, target) {
  
  # D_bar: Get mean Deviance of MCMC 
  dev = 0
  for (i in 1:nrow(model)) {
    params <- model[i,]
    X <- model.matrix(~ predictors, data = data)
    print(X)
    print(params)
    p = inv.logit(X %*% params)
    p[data$target == 0] = 1-p[data$target == 0]
    dev = dev - 2 * sum(log(p))  # Negative log-likelihood
    print(dev)
  }
  D_bar = dev / nrow(model)
  
  # D_theta: Deviance at the posterior mean (using the average parameter values)
  posterior_means <- colMeans(model)
  X <- model.matrix(~ predictors, data = data)
  linear_predictor <- X %*% posterior_means
  p_post <- inv.logit(linear_predictor)
  p_post[data$target == 0] = 1-p_post[data$target == 0]
  
  D_theta = -2 * sum(log(p_post)) # Deviance at the posterior mean
  
  # p_D: Posterior deviance penalty
  p_D = D_bar - D_theta

  # DIC
  DIC = D_bar + p_D
  return(list(DIC=DIC, D_bar=D_bar, p_D=p_D))
}

```







